{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Date: 2019-09-26  \n",
    "Author: skettee  \n",
    "Categories: Reinforcement Learning, Q-Learning    \n",
    "Tags: Environment, Agent, State, Action, Reward, Policy, Q-value, ğœ€-greedy    \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°•í™” í•™ìŠµ(Reinforcement Learning)ì—ì„œ ì‚¬ìš©í•˜ëŠ” Q-Learningì— ëŒ€í•´ì„œ ì•Œì•„ ë³´ê³  Gymì—ì„œ ì œê³µí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ë“¬ì„ ë§Œë“¤ì–´ ë³´ì.  \n",
    "<!--more-->\n",
    "\n",
    "ì‹¤ì œë¡œ ëŒë ¤ ë³´ê³  ì‹¶ìœ¼ë©´ êµ¬ê¸€ ì½”ë©ìœ¼ë¡œ ~  \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skettee/notebooks/blob/master/q_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘¤ ìƒì‚¬\n",
    "\n",
    "> SARSAì—ì„œ ë§ˆì§€ë§‰ Aë¥¼ ìˆ˜í–‰ì„ ì•ˆí•´ë„ ë˜ëŠ” ë°©ë²•ì´ ìˆë‹¤ê³  í•˜ë„¤?        \n",
    "> ì•„ë˜ ì²´ìœ¡ê´€(Gym)ì— ê°€ì„œ      \n",
    "> 'ì–¼ìŒ í˜¸ìˆ˜8X8' ë¬¸ì œë¥¼ ê·¸ê±¸ë¡œ í’€ì–´ ë³´ê²Œ          \n",
    ">\n",
    "> https://gym.openai.com/envs/FrozenLake8x8-v0/\n",
    "\n",
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ë„¤~ ë„¤~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¬¸ì œ ë¶„ì„ (Problem Anaysis)\n",
    "\n",
    "'ì–¼ìŒ í˜¸ìˆ˜8X8'ì€ ì‚´ì‚¬(SARSA)ì—ì„œ ëŒë ¤ ë³´ì•˜ë˜ ë¬¸ì œë‹¤. ì´ë²ˆì—ëŠ” ë¯¸ë„ëŸ¬ ì§€ëŠ” ê²ƒì„ ì¶”ê°€í•´ ë³´ì!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output, Pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Up)\n",
       "SFFFFFFF\n",
       "FFFFFFFF\n",
       "FFFHFFFF\n",
       "FFFFFHFF\n",
       "FFFHFFFF\n",
       "FHHFFFHF\n",
       "\u001b[41mF\u001b[0mHFFHFHF\n",
       "FFFHFFFG\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Environment\n",
    "#\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "state = env.reset()\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "#\n",
    "# Agent\n",
    "#\n",
    "for step in range(100):\n",
    "    action =env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)    \n",
    "    state = next_state\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ê°€ë” ì•¡ì…˜ê³¼ëŠ” ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ê°€ë²„ë¦°ë‹¤.   \n",
    "> ê³¼ì—°  ë¯¸ë„ëŸ¬ì ¸ì„œ êµ¬ë©ì— ë¹ ì§€ì§€ ì•Šê³     \n",
    "> ëª©í‘œì ì— ë„ë‹¬í•  ìˆ˜ ìˆì„ê¹Œ?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ (Environment)\n",
    "\n",
    "'ì–¼ìŒí˜¸ìˆ˜8X8' ì„¸ê³„ì˜ í™˜ê²½ì€ 64ê°œì˜ ìƒíƒœ(State)ì™€ 4ê°œì˜ ì•¡ì…˜(Action)ìœ¼ë¡œ êµ¬ì„± ë˜ì–´ ìˆë‹¤.  ê·¸ë¦¬ê³  ì•„ë˜ë¥¼ ë³´ì    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 47, 0.0, False),\n",
       "  (0.3333333333333333, 54, 0.0, True),\n",
       "  (0.3333333333333333, 63, 1.0, True)],\n",
       " 1: [(0.3333333333333333, 54, 0.0, True),\n",
       "  (0.3333333333333333, 63, 1.0, True),\n",
       "  (0.3333333333333333, 55, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 63, 1.0, True),\n",
       "  (0.3333333333333333, 55, 0.0, False),\n",
       "  (0.3333333333333333, 47, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 55, 0.0, False),\n",
       "  (0.3333333333333333, 47, 0.0, False),\n",
       "  (0.3333333333333333, 54, 0.0, True)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•´ì„ì„ í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "\n",
    "> 55ë²ˆ ìƒíƒœ(state)ì—ì„œ,  \n",
    "> ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ë¼ëŠ” ì•¡ì…˜ì„ ì£¼ë©´, 1/3ì˜ í™•ë¥ ë¡œ ìœ„ë¡œ ì´ë™í•˜ê³ ,  1/3ì˜ í™•ë¥ ë¡œ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ê³ , 1/3ì˜ í™•ë¥ ë¡œ ì•„ë˜ë¡œ ì´ë™í•œë‹¤.  \n",
    "> ì•„ë˜ë¡œ ì´ë™í•˜ë¼ëŠ” ì•¡ì…˜ì„ ì£¼ë©´, 1/3ì˜ í™•ë¥ ë¡œ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ê³ , 1/3ì˜ í™•ë¥ ë¡œ ì•„ë˜ë¡œ ì´ë™í•˜ê³ , 1/3ì˜ í™•ë¥ ë¡œ ê·¸ ìë¦¬ì— ìˆëŠ”ë‹¤.  \n",
    "> ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ë¼ëŠ” ì•¡ì…˜ì„ ì£¼ë©´, 1/3ì˜ í™•ë¥ ë¡œ ì•„ë˜ë¡œ ì´ë™í•˜ê³ , 1/3ì˜ í™•ë¥ ë¡œ ê·¸ìë¦¬ì— ìˆê³ , 1/3ì˜ í™•ë¥ ë¡œ ìœ„ë¡œ ì´ë™í•œë‹¤.  \n",
    "> ì•„ë˜ë¡œ ì´ë™í•˜ë¼ëŠ” ì•¡ì…˜ì„ ì£¼ë©´, 1/3ì˜ í™•ë¥ ë¡œ ê·¸ ìë¦¬ì— ìˆê³ , 1/3ì˜ í™•ë¥ ë¡œ ìœ„ë¡œ ì´ë™í•˜ê³ , 1/3ì˜ í™•ë¥ ë¡œ ì™¼ìª½ìœ¼ë¡œ ì´ë™í•œë‹¤.  \n",
    "\n",
    "ì´ë ‡ê²Œ ë¯¸ë„ëŸ¬ì§ì´ ì¶”ê°€ ë˜ë©´ 1/3ì˜ í™•ë¥ ë¡œ ì •ìƒì ì¸ ì•¡ì…˜ì„ ìˆ˜í–‰í•œë‹¤.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Q-Learning ì˜ ì›ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì •ì±…(policy)ìœ¼ë¡œ í•™ìŠµ ì‹œí‚¨ ë°ì´í„°ë¥¼ ì„ì–´ë„ ìµœì í™”ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.  \n",
    "\n",
    "### SARSA + greedy\n",
    "\n",
    "SARSA + greedy ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œ Q-valueëŠ” ì•„ë˜ì™€ ê°™ë‹¤.  \n",
    "\n",
    "$Q(S_t, A'_t) \\leftarrow Q(S_t, A'_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1}, A'_{t+1}) - Q(S_t, A'_t) \\right)$\n",
    "\n",
    "greedy ë°©ì‹ì€ ë‹¤ìŒ ìŠ¤í…ì˜ ì•¡ì…˜ì„ ì„ íƒí•˜ëŠ” ê²½ìš°ì— ìš•ì‹¬ìŸì´(greedy)ì²˜ëŸ¼ ìµœê³ ì˜ Q-valueì˜ ì•¡ì…˜ë§Œì„ ì„ íƒí•œë‹¤. ë”°ë¼ì„œ ë‹¤ìŒ ì‹ì„ ë§Œì¡±í•œë‹¤.  \n",
    "\n",
    "$A'_{t+1} = \\max_{a'} Q(S_{t+1}, a')$  \n",
    "\n",
    "ë”°ë¼ì„œ Q-valueëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.   \n",
    "\n",
    "$Q(S_t, A'_t) \\leftarrow Q(S_t, A'_t) + \\alpha \\left( R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A'_t) \\right)$\n",
    "\n",
    "### SARSA + ğœ€-greedy\n",
    "\n",
    "SARSA + ğœ€-greedy ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œ Q-valueëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n",
    "\n",
    "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right)$\n",
    "\n",
    "\n",
    "ì—¬ê¸°ì„œ $Q(S_{t+1}, A_{t+1})$ ëŒ€ì‹ ì— SARSA + greedy ë°©ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ $\\max_{a'} Q(S_{t+1}, a')$ì„ ì‚¬ìš©í•œë‹¤.  ê·¸ëŸ¬ë©´...   \n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \\right)$\n",
    "\n",
    "$\\alpha$ : learning rate  \n",
    "$\\gamma$ : ë””ìŠ¤ì¹´ìš´íŠ¸ (discount factor)  \n",
    "\n",
    "#### Target\n",
    "ëª©í‘œë¡œ í•˜ëŠ” ê°’ì´ë‹¤. ì—¬ê¸°ì„œ íƒ€ê²Ÿì€ $R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a')$ì´ë‹¤.  \n",
    "\n",
    "#### Error (ë¸íƒ€)  \n",
    "ëª©í‘œê°’ê³¼ í˜„ì¬ê°’ê³¼ì˜ ì°¨ì´ë¥¼ $\\delta$ ë¼ê³  í•œë‹¤.   \n",
    "\n",
    "$\\delta_t = R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)$\n",
    "\n",
    "ê³„ì†í•´ì„œ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰ ì‹œí‚¤ë©´ì„œ $Q(S_t, A_t)$ì—ë‹¤ê°€ $\\alpha * \\delta_t$ ë¥¼ ì—…ë°ì´íŠ¸ í•˜ë©´ ê²°êµ­ $Q(s, a) \\rightarrow q_*(s,a)$ê°€ ëœë‹¤.  \n",
    "\n",
    "\n",
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> SARSAì—ì„œ ë§ˆì§€ë§‰ Aë¥¼ ìˆ˜í–‰í•  í•„ìš” ì—†ì´     \n",
    "> Q í…Œì´ë¸”ì—ì„œ ê°€ì¥ í° ìˆ˜ì˜ ì•¡ì…˜ì„ ì„ íƒí•˜ë©´   \n",
    "> ìµœì í™”ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ ë°”ë¡œ   \n",
    "> Q-Learningì´ë‹¤!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ (Learning)\n",
    "\n",
    "Q-Learningì„ ì´ìš©í•´ì„œ ìµœì ì˜ Q-valueë¥¼ ì°¾ì•„ë³´ì!   \n",
    "\n",
    "SARSAë³´ë‹¤ ì½”ë“œê°€ ê¹”ë”í•˜ë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:03<00:00, 1390.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_state = env.observation_space.n\n",
    "num_action = env.action_space.n\n",
    "num_episode = 5000\n",
    "\n",
    "# Initialize Q_table \n",
    "Q_table = np.random.uniform(low=0.0, high=0.00000001, size=(num_state, num_action))\n",
    "# Zero for terminate states\n",
    "for s in [19, 29, 35, 41, 42, 49, 52, 54, 59, 63]:\n",
    "    Q_table[s] = 0\n",
    "\n",
    "for episode in tqdm(range(num_episode)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    # Hyper parameter\n",
    "    epsilon = 0.3\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    \n",
    "    while not done:\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        target = reward + gamma*Q_table[next_state, np.argmax(Q_table[next_state])] \n",
    "        delta = target - Q_table[state][action]\n",
    "        Q_table[state][action] += alpha * delta\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•´ê²° (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Right)\n",
       "SFFFFFFF\n",
       "FFFFFFFF\n",
       "FFFHFFFF\n",
       "FFFFFHFF\n",
       "FFFHFFFF\n",
       "FHHFFFHF\n",
       "FHFFHFHF\n",
       "FFFHFFF\u001b[41mG\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ğŸ‰ğŸ‘ ì„±ê³µ! ğŸºğŸ¥‡\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q_table[state]) # Optimal Policy\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done and state == 63:\n",
    "        print('\\n ğŸ‰ğŸ‘ ì„±ê³µ! ğŸºğŸ¥‡')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

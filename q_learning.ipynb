{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Date: 2019-09-26  \n",
    "Author: skettee  \n",
    "Categories: Reinforcement Learning, Q-Learning    \n",
    "Tags: Environment, Agent, State, Action, Reward, Policy, Q-value, 𝜀-greedy    \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습(Reinforcement Learning)에서 사용하는 Q-Learning에 대해서 알아 보고 Gym에서 제공하는 문제를 해결하기 위한 알고리듬을 만들어 보자.  \n",
    "<!--more-->\n",
    "\n",
    "실제로 돌려 보고 싶으면 구글 코랩으로 ~  \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skettee/notebooks/blob/master/q_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👤 상사\n",
    "\n",
    "> SARSA에서 마지막 A를 수행을 안해도 되는 방법이 있다고 하네?        \n",
    "> 아래 체육관(Gym)에 가서      \n",
    "> '얼음 호수8X8' 문제를 그걸로 풀어 보게          \n",
    ">\n",
    "> https://gym.openai.com/envs/FrozenLake8x8-v0/\n",
    "\n",
    "⚙️ 엔지니어\n",
    "\n",
    "> 네~ 네~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 분석 (Problem Anaysis)\n",
    "\n",
    "'얼음 호수8X8'은 살사(SARSA)에서 돌려 보았던 문제다. 이번에는 미끄러 지는 것을 추가해 보자!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output, Pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Down)\n",
       "SFFFFFFF\n",
       "FFFFFFFF\n",
       "FFF\u001b[41mH\u001b[0mFFFF\n",
       "FFFFFHFF\n",
       "FFFHFFFF\n",
       "FHHFFFHF\n",
       "FHFFHFHF\n",
       "FFFHFFFG\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 17 timesteps\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Environment\n",
    "#\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "state = env.reset()\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "#\n",
    "# Agent\n",
    "#\n",
    "for step in range(100):\n",
    "    action =env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)    \n",
    "    state = next_state\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚙️ 엔지니어\n",
    "\n",
    "> 가끔 액션과는 다른 방향으로 가버린다.   \n",
    "> 과연  미끄러져서 구멍에 빠지지 않고    \n",
    "> 목표점에 도달할 수 있을까?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 (Environment)\n",
    "\n",
    "'얼음호수8X8' 세계의 환경은 64개의 상태(State)와 4개의 액션(Action)으로 구성 되어 있다.  그리고 아래를 보자    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 47, 0.0, False),\n",
       "  (0.3333333333333333, 54, 0.0, True),\n",
       "  (0.3333333333333333, 63, 1.0, True)],\n",
       " 1: [(0.3333333333333333, 54, 0.0, True),\n",
       "  (0.3333333333333333, 63, 1.0, True),\n",
       "  (0.3333333333333333, 55, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 63, 1.0, True),\n",
       "  (0.3333333333333333, 55, 0.0, False),\n",
       "  (0.3333333333333333, 47, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 55, 0.0, False),\n",
       "  (0.3333333333333333, 47, 0.0, False),\n",
       "  (0.3333333333333333, 54, 0.0, True)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해석을 해보면 다음과 같다.\n",
    "\n",
    "> 55번 상태(state)에서,  \n",
    "> 왼쪽으로 이동하라는 액션을 주면, 1/3의 확률로 위로 이동하고,  1/3의 확률로 왼쪽으로 이동하고, 1/3의 확률로 아래로 이동한다.  \n",
    "> 아래로 이동하라는 액션을 주면, 1/3의 확률로 왼쪽으로 이동하고, 1/3의 확률로 아래로 이동하고, 1/3의 확률로 그 자리에 있는다.  \n",
    "> 오른쪽으로 이동하라는 액션을 주면, 1/3의 확률로 아래로 이동하고, 1/3의 확률로 그자리에 있고, 1/3의 확률로 위로 이동한다.  \n",
    "> 아래로 이동하라는 액션을 주면, 1/3의 확률로 그 자리에 있고, 1/3의 확률로 위로 이동하고, 1/3의 확률로 왼쪽으로 이동한다.  \n",
    "\n",
    "이렇게 미끄러짐이 추가 되면 1/3의 확률로 정상적인 액션을 수행한다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Q-Learning 의 원리는 살사(SARSA)에서 서로 다른 정책(policy)으로 훈련 시킨 데이터를 섞어도 최적화가 가능하다는 것이다.  \n",
    "\n",
    "일단, SARSA + greedy 방식으로 훈련해서 $Q(S, A')$을 만들어 놓는다.\n",
    "\n",
    "그리고 우리는 SARSA + 𝜀-greedy 방식으로 훈련해서 $Q(S, A)$를 만들 것이다.  이렇게...\n",
    "\n",
    "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right)$\n",
    "\n",
    "여기서 $Q(S_{t+1}, A_{t+1})$ 대신에 SARSA + greedy 방식으로 만들어진 $Q(S_{t+1}, A')$을 사용한다.  이렇게...  \n",
    "\n",
    "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t) \\right)$\n",
    "\n",
    "A' 은 greedy 방식으로 만들어 졌으므로 다음의 식이 성립한다.  \n",
    "\n",
    "$\\pi(S_{t+1}) = \\text{argmax}_{a'} Q(S_{t+1}, a')$\n",
    "\n",
    "$\\pi(S_{t+1})$를 A' 에 박아 넣으면 다음의 식이 나온다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathsf {Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left( R + \\gamma \\max_{a'} Q(S', a') - Q(S, A) \\right)}$\n",
    "\n",
    "$\\alpha$ : learning rate  \n",
    "$\\gamma$ : 디스카운트 (discount factor)  \n",
    "\n",
    "\n",
    "⚙️ 엔지니어\n",
    "\n",
    "> SARSA에서 마지막 A를 수행할 필요 없이     \n",
    "> Q 테이블에서 가장 높을 점수의 액션을 선택하면   \n",
    "> 최적화가 가능하다는 것이 바로   \n",
    "> Q-Learning이다!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 (Learning)\n",
    "\n",
    "Q-Learning을 이용해서 최적의 Q-value를 찾아보자!   \n",
    "\n",
    "SARSA보다 코드가 깔끔하다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 1558.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_state = env.observation_space.n\n",
    "num_action = env.action_space.n\n",
    "num_episode = 5000\n",
    "\n",
    "# Initialize Q_table \n",
    "Q_table = np.random.uniform(low=0.0, high=0.00000001, size=(num_state, num_action))\n",
    "# Zero for terminate states\n",
    "for s in [19, 29, 35, 41, 42, 49, 52, 54, 59, 63]:\n",
    "    Q_table[s] = 0\n",
    "\n",
    "for episode in tqdm(range(num_episode)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    # Hyper parameter\n",
    "    epsilon = 0.3\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    \n",
    "    while not done:\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        Q_table[state][action] += alpha * (reward + gamma*Q_table[next_state, np.argmax(Q_table[next_state])] \\\n",
    "                                           - Q_table[state][action])\n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해결 (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Right)\n",
       "SFFFFFFF\n",
       "FFFFFFFF\n",
       "FFFHFFFF\n",
       "FFFFFHFF\n",
       "FFFHFFFF\n",
       "FHHFFFHF\n",
       "FHFFHFHF\n",
       "FFFHFFF\u001b[41mG\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 🎉👍 성공! 🍺🥇\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q_table[state]) # Optimal Policy\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done and state == 63:\n",
    "        print('\\n 🎉👍 성공! 🍺🥇')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

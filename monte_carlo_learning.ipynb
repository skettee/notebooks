{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Learning\n",
    "\n",
    "Date: 2019-09-23  \n",
    "Author: skettee  \n",
    "Categories: Reinforcement Learning, Monte-Carlo Learning   \n",
    "Tags: Environment, Agent, State, Action, Reward, Policy, Q-value, Monte-Carlo  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습(Reinforcement Learning)에서 사용하는 Monte Carlo에 대해서 알아 보고 Gym에서 제공하는 문제를 해결하기 위한 알고리듬을 만들어 보자.  \n",
    "<!--more-->\n",
    "\n",
    "실제로 돌려 보고 싶으면 구글 코랩으로 ~  \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skettee/notebooks/blob/master/monte_carlo_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 (Problem)\n",
    "\n",
    "👤 상사\n",
    "\n",
    "> 인공지능 게임 프로젝트를 시작한다.    \n",
    "> 게임을 만들려면 '강화학습'이라는 것을 알아야 한다는데...     \n",
    "> 아래 체육관(Gym)에 가서     \n",
    "> '얼음 호수' 문제를 한번 풀어보게        \n",
    ">\n",
    "> https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "⚙️ 엔지니어\n",
    "\n",
    "> 오~ 게임이라     \n",
    "> 딥러닝에 찌든 머리가 갑자기 맑아진다~     \n",
    "> 새로운 마음으로 문제를 해결해 보자!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 분석 (Problem Anaysis)\n",
    "\n",
    "일단 Gym을 설치하고 돌려 보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output, Pretty\n",
    "\n",
    "\n",
    "def get_optimal_value(state, action, reward):\n",
    "    return None\n",
    "\n",
    "def get_optimal_action():\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Down)\n",
       "SFFF\n",
       "F\u001b[41mH\u001b[0mFH\n",
       "FFFH\n",
       "HFFG\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Environment\n",
    "#\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False) # 얼음위에서 미끄러지지 않도록 설정\n",
    "state = env.reset()\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "#\n",
    "# Agent\n",
    "#\n",
    "for step in range(100):\n",
    "    action = get_optimal_action()\n",
    "    next_state, reward, done, info = env.step(action)    \n",
    "    get_optimal_value(state, action, reward)\n",
    "    state = next_state\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4X4 매트릭스에서 `S`(Start)에서 시작해서 `H`(Hole)를 피해서 `G`(Goal)까지 도착하는 최적의 길을 찾는 게임이다.  코드를 보면 ... \n",
    "\n",
    "1. 환경(Environemnt)과 에이젼트(Agent)로 나누어져 있고,  \n",
    "2. 에이젼트는 환경으로 부터 상태(state)를 얻어 오고  \n",
    "3. 최적의 액션(action)을 결정하고   \n",
    "4. 액션을 실행하고 그 결과로 다음 상태, 보상(Reward)을 얻어 오고  \n",
    "5. 상태, 액션, 보상을 이용해서 최적의 가치(Value)를 계산하고   \n",
    "6. 3,4,5를 반복하는   \n",
    "\n",
    "... 구조로 되어 있다.\n",
    "\n",
    "⚙️ 엔지니어\n",
    "\n",
    "> 이거...   \n",
    "> 딥러닝하고는 완전 다른 문제다...  \n",
    "> 딥러닝은 데이터를 주고 사람보다 뛰어난 분류와 예측을 하도록 **모델링** 을 하는 것이다.    \n",
    "> 강화학습은 환경과 에이젼트를 주고 사람보다 뛰어난 행동을 하도록 **알고리듬** 을 만드는 것이다.  \n",
    "> \n",
    "> 우선 환경(Environment)에 대해서 알아 보자  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 (Environment)\n",
    "\n",
    "'얼음호수'의 환경은 16개의 상태(State)와 4개의 액션(Action)으로 구성 되어 있다.  \n",
    "\n",
    "### 액션 (Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action Space\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'얼음호수'에서는 4개의 액션이 존재한다. 그리고 각각의 액션이 번호로 지정 되어 있다.\n",
    "\n",
    "$A = \\{0, 1, 2, 3\\}$   \n",
    "\n",
    "Num\t| Action\n",
    "----|----\n",
    "0 |\t왼쪽으로 이동 (Move Left)\n",
    "1 |\t아래로 이동 (Move Down)\n",
    "2 |\t오른쪽으로 이동 (Move Right)\n",
    "3 |\t위로 이동 (Move Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상태 (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# State Space\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'얼음호수'의 상태(State) $S$는 다음과 같이 각 상태가 0과 15까지 번호로 지정되어 있다.\n",
    "\n",
    "$S = \\{0, 1, \\cdots , 15\\}$   \n",
    "\n",
    "$\\begin{vmatrix}\n",
    "0 & 1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 & 7 \\\\\n",
    "8 & 9 & 10 & 11 \\\\\n",
    "12 & 13 & 14 & 15\n",
    "\\end{vmatrix}$\n",
    "\n",
    "그리고 각 상태마다 액션(action), 확률(probability), 다음 상태(next state), 보상(reward), 종료(done)가 `{action: [(probability, nextstate, reward, done)]}` 형식으로 정의 되어 있다.   \n",
    "\n",
    "6번 상태를 까보자   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 5, 0.0, True)],\n",
       " 1: [(1.0, 10, 0.0, False)],\n",
       " 2: [(1.0, 7, 0.0, True)],\n",
       " 3: [(1.0, 2, 0.0, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해석을 해 보면 다음과 같다.  \n",
    "\n",
    "> 6번 상태에서  \n",
    "> 왼쪽으로 이동하면 100%의 확률로 5번 상태로 변환되고, 보상은 0.0, 종료한다.  \n",
    "> 아래로 이동하면 100%의 확률로 10번 상태로 변환되고, 보상은 0.0, 계속한다.  \n",
    "> 오른쪽으로 이동하면 100%의 확률로 7번 상태로 변환되고 보상은 0.0, 종료한다.  \n",
    "> 위로 이동하면 100%의 확률로 2번 상태로 변환되고 보상은 0.0, 계속한다.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보상 (Reward)\n",
    "\n",
    "14번의 상태도 까보자  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 13, 0.0, False)],\n",
       " 1: [(1.0, 14, 0.0, False)],\n",
       " 2: [(1.0, 15, 1.0, True)],\n",
       " 3: [(1.0, 10, 0.0, False)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14번 상태에서 오른쪽으로 이동하면 100% 확률로 15번 상태로 변환되고 1.0을 보상 받고 종료한다! \n",
    "\n",
    "기대 보상값은 아래와 같이 정의한다.  \n",
    "\n",
    "$\\mathcal R_s^a = \\mathbb E [\\mathcal R_{t+1} | S_t = s, A_t = a ]$\n",
    "\n",
    "예를 들어서 14번 상태에서, 2번 액션에서의 기대 보상값은 아래와 같다.  \n",
    "\n",
    "$\\mathcal R_{14}^{2} = 1.0$ \n",
    "\n",
    "#### Return (미래 보상의 합)  \n",
    "\n",
    "에이전트는 바로 다음의 보상만을 보고 액션을 하면 안된다.  먼 미래의 최종 보상의 합을 예상해서 액션을 해야 한다.  \n",
    "'2보 전진을 위한 1보 후퇴' 전략을 구사할 수 있어야 한다.  \n",
    "\n",
    "에이전트가 원하는 최종 골(Goal)은 다음의 식과 같이 다음, 다음, 다음...의 미래의 보상들의 합을 돌려 받는(Return) 것이다.  \n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$\n",
    "\n",
    "그러나 에이전트도 욕심이 있어서 바로 다음 보상이 커보인다. 이것을 만족시켜 주는 것이 디스카운트(discount factor) $\\gamma$ 값이다.  $\\gamma$ 는 0과 1사이의 값을 가진다. 이렇게 하면 미래로 갈 수록 보상은 지수적으로 작아 보이게 된다.\n",
    "\n",
    "$\\gamma \\in [0,1]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚙️ 엔지니어\n",
    "\n",
    "> 에이전트는    \n",
    "> 보상을 받기 위해   \n",
    "> 구멍에 빠지지 않고 15번 상태로 이동하기 위해   \n",
    "> 최적의 액션을 해야 한다.   \n",
    ">  \n",
    "> 우리는  \n",
    "> 에이전트가 최적의 액션을 하도록   \n",
    "> 알고리듬을 만들어야 한다.  \n",
    "> 어떻게 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 (Agent)\n",
    "\n",
    "에이전트는 환경에서 부터 주어진 상태($S$), 액션($A$), 보상($R$)을 가지고 가치(Q-value)를 계산하고 정책(Policy)을 수립해서 최적의 액션을 수행해야 한다.  \n",
    "\n",
    "### 정책 (Policy)\n",
    "\n",
    "에이전트가 주어진 상태에서 액션을 선택하는 확률이다.  \n",
    "\n",
    "$\\pi(s,a) = \\pi (a|s) = \\mathbb P[A_t = a | S_t = s]$ \n",
    "\n",
    "예를 들어서 $\\pi (1 | 5) = 0.25$ 라는 것은 5번 상태에서 아래로 이동하는 액션(1) 을 수행할 확률을 25%로 정한다는 뜻이다.  \n",
    "\n",
    "### 상태 가치 (State Value)\n",
    "\n",
    "상태 가치는 상태 s 에서 기대되는 미래 보상의 합이다.  \n",
    "\n",
    "$v_{\\pi}(s) = \\mathbb E_{\\pi} [G_t | S_t = s]$ \n",
    "\n",
    "### Q-value (State Action Value)\n",
    "\n",
    "Q-value는 상태 s 에서 액션 a 를 수행할 경우에 기대되는 미래 보상의 합이다.  \n",
    "\n",
    "$q_{\\pi}(s,a) = \\mathbb E_{\\pi} [G_t | S_t = s, A_t = a]$ \n",
    "\n",
    "### 최적 가치 (Optimal Value)\n",
    "\n",
    "상태 s 에서 가장 큰 Q-value이다.       \n",
    "\n",
    "$q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a)$  \n",
    "\n",
    "### 최적 정책 (Optimal Policy)\n",
    "\n",
    "$q_*(s,a)$가 결정 되면 $\\pi (s, a) = 1$ 이 된다. 즉 상태 s일때는 100% 액션 a를 수행 한다.\n",
    "\n",
    "$ \\pi_*(s, a) = \\begin{cases}\n",
    "1 & \\text{if } a= \\text{argmax}_{a \\in A} q_\\star(s,a) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚙️ 엔지니어\n",
    "\n",
    "> 에이전트는      \n",
    "> 각 상태(state)의 액션(action)마다  \n",
    "> Q-value를 계산한다.  \n",
    ">  \n",
    "> 에이전트가\n",
    "> 최적의 액션을 하기 위해서는  \n",
    "> 각 상태에서 Q-value의 최고값을 가진  \n",
    "> 액션을 수행하면 된다!  \n",
    ">\n",
    "> 그런데  \n",
    "> Q-value를 어떻게 찾지?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메모리 (Memory)\n",
    "\n",
    "랜덤 액션에서 얻은 데이터 ($s_t, a_t$) 들을 몬테 카를로에서 사용하기 위해서 메모리(memory)에 저장한다.\n",
    "\n",
    "$\\text{memory} = [(s_0, a_0), (s_1, a_1), \\cdots]$\n",
    "\n",
    "### Monte Carlo (몬테 카를로)\n",
    "\n",
    "'얼음호수' 에서 `S`(Start)에서 시작해서 `H`(Hole)에 빠지거나  `G`(Goal)에 도착하면 게임을 끝이난다. 이 것을 **에피소드(episode)** 라고 한다.  하나의 에피소드가 끝나면 에피소드에서 지나왔던 상태(state)에 대해서 Q-value를 구한다. 그리고 계속해서 에피소드를 실행 시키면서 지나온 상태(state)에 대한 Q-value를 업데이트 하면 결국 최적의 Q-value를 구할 수 있다. 이것이 몬테 카를로 방법(Monte Carlo method)이다. 이때 액션은 랜덤으로 결정한다.  \n",
    "\n",
    "$\\pi(0|\\cdot) = \\pi(1|\\cdot) = \\pi(2|\\cdot) = \\pi(3|\\cdot) = 0.25$\n",
    "\n",
    "하나의 에피소드가 끝나면 아래와 같이 Q-value를 구한다.  \n",
    "\n",
    "$k$: Sample k번째 에피소드  \n",
    "\n",
    "$\\begin{align}\n",
    "N(S_t, A_t) & \\leftarrow N(S_t, A_t) + 1 \\\\\n",
    "Q(S_t, A_t) & \\leftarrow Q(S_t, A_t) + \\dfrac{1}{N(S_t, A_t)} \\left( G_t - Q(S_t, A_t) \\right)  \n",
    "\\end{align}$\n",
    "\n",
    "#### Target  \n",
    "\n",
    "목표로 하는 값이다. 여기서 타겟은 $G_t$이다.  \n",
    "\n",
    "#### Error (델타)  \n",
    "\n",
    "목표값과 현재값과의 차이를 $\\delta$ 라고 한다.   \n",
    "$\\delta_t = G_t - Q(S_t, A_t) $\n",
    "\n",
    "계속해서 에피소드를 실행 시키면서 $Q(S_t, A_t)$에다가 $\\delta_t$ 의 평균을 업데이트 하면 결국 $Q(s, a) \\rightarrow q_*(s,a)$가 된다.  \n",
    "\n",
    "\n",
    "⚙️ 엔지니어\n",
    "\n",
    "> 아하! 최적의 Q-value를  찾아 가는 것이 강화 학습(Reinfocement Learning)하는 방법이구나!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 (Learning)\n",
    "\n",
    "몬테 카를로를 이용해서 최적의 Q-value를 찾아보자!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 7738.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_state = env.observation_space.n\n",
    "num_action = env.action_space.n\n",
    "num_episode = 5000\n",
    "\n",
    "# Q_table\n",
    "Q_table = np.zeros((num_state, num_action))\n",
    "# N_table\n",
    "N_table = np.zeros((num_state, num_action))\n",
    "# R_table\n",
    "R_table = np.zeros((num_state, num_action))\n",
    "\n",
    "for episode in tqdm(range(num_episode)):\n",
    "    memory = []    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(100):\n",
    "        action = env.action_space.sample()        \n",
    "        memory.append((state, action)) # trajectory\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        R_table[state][action] = reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done: # an episode finished  \n",
    "            #\n",
    "            # Monte Carlo policy evaluation\n",
    "            #\n",
    "            G_t = 0\n",
    "            for i in range(len(memory)):\n",
    "                gamma = 0.6 # discount factor\n",
    "                for j in range( i, len(memory) ):\n",
    "                    S_t = memory[j][0]\n",
    "                    A_t = memory[j][1]\n",
    "                    if i==j:\n",
    "                        G_t += R_table[S_t][A_t]\n",
    "                    else:\n",
    "                        G_t += gamma * R_table[S_t][A_t]\n",
    "                        gamma = gamma * gamma\n",
    "                S_t = memory[i][0]\n",
    "                A_t = memory[i][1]\n",
    "                N_table[S_t][A_t] += 1\n",
    "                Q_table[S_t][A_t] += (G_t - Q_table[S_t][A_t]) / N_table[S_t][A_t]\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해결 (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Right)\n",
       "SFFF\n",
       "FHFH\n",
       "FFFH\n",
       "HFF\u001b[41mG\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 🎉👍 성공! 🍺🥇\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q_table[state]) # Optimal Policy\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done and state == 15:\n",
    "        print('\\n 🎉👍 성공! 🍺🥇')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

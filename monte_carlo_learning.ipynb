{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Learning\n",
    "\n",
    "Date: 2019-09-23  \n",
    "Author: skettee  \n",
    "Categories: Reinforcement Learning, Monte-Carlo Learning   \n",
    "Tags: Environment, Agent, State, Action, Reward, Policy, Q-value, Monte-Carlo  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°•í™” í•™ìŠµ(Reinforcement Learning)ì—ì„œ ì‚¬ìš©í•˜ëŠ” Monte Carloì— ëŒ€í•´ì„œ ì•Œì•„ ë³´ê³  Gymì—ì„œ ì œê³µí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ë“¬ì„ ë§Œë“¤ì–´ ë³´ì.  \n",
    "<!--more-->\n",
    "\n",
    "ì‹¤ì œë¡œ ëŒë ¤ ë³´ê³  ì‹¶ìœ¼ë©´ êµ¬ê¸€ ì½”ë©ìœ¼ë¡œ ~  \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skettee/notebooks/blob/master/monte_carlo_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¬¸ì œ (Problem)\n",
    "\n",
    "ğŸ‘¤ ìƒì‚¬\n",
    "\n",
    "> ì¸ê³µì§€ëŠ¥ ê²Œì„ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•œë‹¤.    \n",
    "> ê²Œì„ì„ ë§Œë“¤ë ¤ë©´ 'ê°•í™”í•™ìŠµ'ì´ë¼ëŠ” ê²ƒì„ ì•Œì•„ì•¼ í•œë‹¤ëŠ”ë°...     \n",
    "> ì•„ë˜ ì²´ìœ¡ê´€(Gym)ì— ê°€ì„œ     \n",
    "> 'ì–¼ìŒ í˜¸ìˆ˜' ë¬¸ì œë¥¼ í•œë²ˆ í’€ì–´ë³´ê²Œ        \n",
    ">\n",
    "> https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ì˜¤~ ê²Œì„ì´ë¼     \n",
    "> ë”¥ëŸ¬ë‹ì— ì°Œë“  ë¨¸ë¦¬ê°€ ê°‘ìê¸° ë§‘ì•„ì§„ë‹¤~     \n",
    "> ìƒˆë¡œìš´ ë§ˆìŒìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•´ ë³´ì!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¬¸ì œ ë¶„ì„ (Problem Anaysis)\n",
    "\n",
    "ì¼ë‹¨ Gymì„ ì„¤ì¹˜í•˜ê³  ëŒë ¤ ë³´ì!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output, Pretty\n",
    "\n",
    "\n",
    "def get_optimal_value(state, action, reward):\n",
    "    return None\n",
    "\n",
    "def get_optimal_action():\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Down)\n",
       "SFFF\n",
       "F\u001b[41mH\u001b[0mFH\n",
       "FFFH\n",
       "HFFG\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 6 timesteps\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Environment\n",
    "#\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False) # ì–¼ìŒìœ„ì—ì„œ ë¯¸ë„ëŸ¬ì§€ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "state = env.reset()\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "#\n",
    "# Agent\n",
    "#\n",
    "for step in range(100):\n",
    "    action = get_optimal_action()\n",
    "    next_state, reward, done, info = env.step(action)    \n",
    "    get_optimal_value(state, action, reward)\n",
    "    state = next_state\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4X4 ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ `S`(Start)ì—ì„œ ì‹œì‘í•´ì„œ `H`(Hole)ë¥¼ í”¼í•´ì„œ `G`(Goal)ê¹Œì§€ ë„ì°©í•˜ëŠ” ìµœì ì˜ ê¸¸ì„ ì°¾ëŠ” ê²Œì„ì´ë‹¤.  ì½”ë“œë¥¼ ë³´ë©´ ... \n",
    "\n",
    "1. í™˜ê²½(Environemnt)ê³¼ ì—ì´ì ¼íŠ¸(Agent)ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆê³ ,  \n",
    "2. ì—ì´ì ¼íŠ¸ëŠ” í™˜ê²½ìœ¼ë¡œ ë¶€í„° ìƒíƒœ(state)ë¥¼ ì–»ì–´ ì˜¤ê³   \n",
    "3. ìµœì ì˜ ì•¡ì…˜(action)ì„ ê²°ì •í•˜ê³    \n",
    "4. ì•¡ì…˜ì„ ì‹¤í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¡œ ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ(Reward)ì„ ì–»ì–´ ì˜¤ê³   \n",
    "5. ìƒíƒœ, ì•¡ì…˜, ë³´ìƒì„ ì´ìš©í•´ì„œ ìµœì ì˜ ê°€ì¹˜(Value)ë¥¼ ê³„ì‚°í•˜ê³    \n",
    "6. 3,4,5ë¥¼ ë°˜ë³µí•˜ëŠ”   \n",
    "\n",
    "... êµ¬ì¡°ë¡œ ë˜ì–´ ìˆë‹¤.\n",
    "\n",
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ì´ê±°...   \n",
    "> ë”¥ëŸ¬ë‹í•˜ê³ ëŠ” ì™„ì „ ë‹¤ë¥¸ ë¬¸ì œë‹¤...  \n",
    "> ë”¥ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ ì£¼ê³  ì‚¬ëŒë³´ë‹¤ ë›°ì–´ë‚œ ë¶„ë¥˜ì™€ ì˜ˆì¸¡ì„ í•˜ë„ë¡ **ëª¨ë¸ë§** ì„ í•˜ëŠ” ê²ƒì´ë‹¤.    \n",
    "> ê°•í™”í•™ìŠµì€ í™˜ê²½ê³¼ ì—ì´ì ¼íŠ¸ë¥¼ ì£¼ê³  ì‚¬ëŒë³´ë‹¤ ë›°ì–´ë‚œ í–‰ë™ì„ í•˜ë„ë¡ **ì•Œê³ ë¦¬ë“¬** ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤.  \n",
    "> \n",
    "> ìš°ì„  í™˜ê²½(Environment)ì— ëŒ€í•´ì„œ ì•Œì•„ ë³´ì  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ (Environment)\n",
    "\n",
    "'ì–¼ìŒí˜¸ìˆ˜'ì˜ í™˜ê²½ì€ 16ê°œì˜ ìƒíƒœ(State)ì™€ 4ê°œì˜ ì•¡ì…˜(Action)ìœ¼ë¡œ êµ¬ì„± ë˜ì–´ ìˆë‹¤.  \n",
    "\n",
    "### ì•¡ì…˜ (Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action Space\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ì–¼ìŒí˜¸ìˆ˜'ì—ì„œëŠ” 4ê°œì˜ ì•¡ì…˜ì´ ì¡´ì¬í•œë‹¤. ê·¸ë¦¬ê³  ê°ê°ì˜ ì•¡ì…˜ì´ ë²ˆí˜¸ë¡œ ì§€ì • ë˜ì–´ ìˆë‹¤.\n",
    "\n",
    "$A = \\{0, 1, 2, 3\\}$   \n",
    "\n",
    "Num\t| Action\n",
    "----|----\n",
    "0 |\tì™¼ìª½ìœ¼ë¡œ ì´ë™ (Move Left)\n",
    "1 |\tì•„ë˜ë¡œ ì´ë™ (Move Down)\n",
    "2 |\tì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ (Move Right)\n",
    "3 |\tìœ„ë¡œ ì´ë™ (Move Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìƒíƒœ (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# State Space\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ì–¼ìŒí˜¸ìˆ˜'ì˜ ìƒíƒœ(State) $S$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê° ìƒíƒœê°€ 0ê³¼ 15ê¹Œì§€ ë²ˆí˜¸ë¡œ ì§€ì •ë˜ì–´ ìˆë‹¤.\n",
    "\n",
    "$S = \\{0, 1, \\cdots , 15\\}$   \n",
    "\n",
    "$\\begin{vmatrix}\n",
    "0 & 1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 & 7 \\\\\n",
    "8 & 9 & 10 & 11 \\\\\n",
    "12 & 13 & 14 & 15\n",
    "\\end{vmatrix}$\n",
    "\n",
    "ê·¸ë¦¬ê³  ê° ìƒíƒœë§ˆë‹¤ ì•¡ì…˜(action), í™•ë¥ (probability), ë‹¤ìŒ ìƒíƒœ(next state), ë³´ìƒ(reward), ì¢…ë£Œ(done)ê°€ `{action: [(probability, nextstate, reward, done)]}` í˜•ì‹ìœ¼ë¡œ ì •ì˜ ë˜ì–´ ìˆë‹¤.   \n",
    "\n",
    "6ë²ˆ ìƒíƒœë¥¼ ê¹Œë³´ì   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 5, 0.0, True)],\n",
       " 1: [(1.0, 10, 0.0, False)],\n",
       " 2: [(1.0, 7, 0.0, True)],\n",
       " 3: [(1.0, 2, 0.0, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•´ì„ì„ í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n",
    "\n",
    "> 6ë²ˆ ìƒíƒœì—ì„œ  \n",
    "> ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ë©´ 100%ì˜ í™•ë¥ ë¡œ 5ë²ˆ ìƒíƒœë¡œ ë³€í™˜ë˜ê³ , ë³´ìƒì€ 0.0, ì¢…ë£Œí•œë‹¤.  \n",
    "> ì•„ë˜ë¡œ ì´ë™í•˜ë©´ 100%ì˜ í™•ë¥ ë¡œ 10ë²ˆ ìƒíƒœë¡œ ë³€í™˜ë˜ê³ , ë³´ìƒì€ 0.0, ê³„ì†í•œë‹¤.  \n",
    "> ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ë©´ 100%ì˜ í™•ë¥ ë¡œ 7ë²ˆ ìƒíƒœë¡œ ë³€í™˜ë˜ê³  ë³´ìƒì€ 0.0, ì¢…ë£Œí•œë‹¤.  \n",
    "> ìœ„ë¡œ ì´ë™í•˜ë©´ 100%ì˜ í™•ë¥ ë¡œ 2ë²ˆ ìƒíƒœë¡œ ë³€í™˜ë˜ê³  ë³´ìƒì€ 0.0, ê³„ì†í•œë‹¤.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë³´ìƒ (Reward)\n",
    "\n",
    "14ë²ˆì˜ ìƒíƒœë„ ê¹Œë³´ì  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 13, 0.0, False)],\n",
       " 1: [(1.0, 14, 0.0, False)],\n",
       " 2: [(1.0, 15, 1.0, True)],\n",
       " 3: [(1.0, 10, 0.0, False)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14ë²ˆ ìƒíƒœì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ë©´ 100% í™•ë¥ ë¡œ 15ë²ˆ ìƒíƒœë¡œ ë³€í™˜ë˜ê³  1.0ì„ ë³´ìƒ ë°›ê³  ì¢…ë£Œí•œë‹¤! \n",
    "\n",
    "ê¸°ëŒ€ ë³´ìƒê°’ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•œë‹¤.  \n",
    "\n",
    "$\\mathcal R_s^a = \\mathbb E [\\mathcal R_{t+1} | S_t = s, A_t = a ]$\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ì„œ 14ë²ˆ ìƒíƒœì—ì„œ, 2ë²ˆ ì•¡ì…˜ì—ì„œì˜ ê¸°ëŒ€ ë³´ìƒê°’ì€ ì•„ë˜ì™€ ê°™ë‹¤.  \n",
    "\n",
    "$\\mathcal R_{14}^{2} = 1.0$ \n",
    "\n",
    "#### Return (ë¯¸ë˜ ë³´ìƒì˜ í•©)  \n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” ë°”ë¡œ ë‹¤ìŒì˜ ë³´ìƒë§Œì„ ë³´ê³  ì•¡ì…˜ì„ í•˜ë©´ ì•ˆëœë‹¤.  ë¨¼ ë¯¸ë˜ì˜ ìµœì¢… ë³´ìƒì˜ í•©ì„ ì˜ˆìƒí•´ì„œ ì•¡ì…˜ì„ í•´ì•¼ í•œë‹¤.  \n",
    "'2ë³´ ì „ì§„ì„ ìœ„í•œ 1ë³´ í›„í‡´' ì „ëµì„ êµ¬ì‚¬í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.  \n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì›í•˜ëŠ” ìµœì¢… ê³¨(Goal)ì€ ë‹¤ìŒì˜ ì‹ê³¼ ê°™ì´ ë‹¤ìŒ, ë‹¤ìŒ, ë‹¤ìŒ...ì˜ ë¯¸ë˜ì˜ ë³´ìƒë“¤ì˜ í•©ì„ ëŒë ¤ ë°›ëŠ”(Return) ê²ƒì´ë‹¤.  \n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ ì—ì´ì „íŠ¸ë„ ìš•ì‹¬ì´ ìˆì–´ì„œ ë°”ë¡œ ë‹¤ìŒ ë³´ìƒì´ ì»¤ë³´ì¸ë‹¤. ì´ê²ƒì„ ë§Œì¡±ì‹œì¼œ ì£¼ëŠ” ê²ƒì´ ë””ìŠ¤ì¹´ìš´íŠ¸(discount factor) $\\gamma$ ê°’ì´ë‹¤.  $\\gamma$ ëŠ” 0ê³¼ 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§„ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë¯¸ë˜ë¡œ ê°ˆ ìˆ˜ë¡ ë³´ìƒì€ ì§€ìˆ˜ì ìœ¼ë¡œ ì‘ì•„ ë³´ì´ê²Œ ëœë‹¤.\n",
    "\n",
    "$\\gamma \\in [0,1]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ì—ì´ì „íŠ¸ëŠ”    \n",
    "> ë³´ìƒì„ ë°›ê¸° ìœ„í•´   \n",
    "> êµ¬ë©ì— ë¹ ì§€ì§€ ì•Šê³  15ë²ˆ ìƒíƒœë¡œ ì´ë™í•˜ê¸° ìœ„í•´   \n",
    "> ìµœì ì˜ ì•¡ì…˜ì„ í•´ì•¼ í•œë‹¤.   \n",
    ">  \n",
    "> ìš°ë¦¬ëŠ”  \n",
    "> ì—ì´ì „íŠ¸ê°€ ìµœì ì˜ ì•¡ì…˜ì„ í•˜ë„ë¡   \n",
    "> ì•Œê³ ë¦¬ë“¬ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤.  \n",
    "> ì–´ë–»ê²Œ ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—ì´ì „íŠ¸ (Agent)\n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” í™˜ê²½ì—ì„œ ë¶€í„° ì£¼ì–´ì§„ ìƒíƒœ($S$), ì•¡ì…˜($A$), ë³´ìƒ($R$)ì„ ê°€ì§€ê³  ê°€ì¹˜(Q-value)ë¥¼ ê³„ì‚°í•˜ê³  ì •ì±…(Policy)ì„ ìˆ˜ë¦½í•´ì„œ ìµœì ì˜ ì•¡ì…˜ì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.  \n",
    "\n",
    "### ì •ì±… (Policy)\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì£¼ì–´ì§„ ìƒíƒœì—ì„œ ì•¡ì…˜ì„ ì„ íƒí•˜ëŠ” í™•ë¥ ì´ë‹¤.  \n",
    "\n",
    "$\\pi(s,a) = \\pi (a|s) = \\mathbb P[A_t = a | S_t = s]$ \n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ì„œ $\\pi (1 | 5) = 0.25$ ë¼ëŠ” ê²ƒì€ 5ë²ˆ ìƒíƒœì—ì„œ ì•„ë˜ë¡œ ì´ë™í•˜ëŠ” ì•¡ì…˜(1) ì„ ìˆ˜í–‰í•  í™•ë¥ ì„ 25%ë¡œ ì •í•œë‹¤ëŠ” ëœ»ì´ë‹¤.  \n",
    "\n",
    "### ìƒíƒœ ê°€ì¹˜ (State Value)\n",
    "\n",
    "ìƒíƒœ ê°€ì¹˜ëŠ” ìƒíƒœ s ì—ì„œ ê¸°ëŒ€ë˜ëŠ” ë¯¸ë˜ ë³´ìƒì˜ í•©ì´ë‹¤.  \n",
    "\n",
    "$v_{\\pi}(s) = \\mathbb E_{\\pi} [G_t | S_t = s]$ \n",
    "\n",
    "### Q-value (State Action Value)\n",
    "\n",
    "Q-valueëŠ” ìƒíƒœ s ì—ì„œ ì•¡ì…˜ a ë¥¼ ìˆ˜í–‰í•  ê²½ìš°ì— ê¸°ëŒ€ë˜ëŠ” ë¯¸ë˜ ë³´ìƒì˜ í•©ì´ë‹¤.  \n",
    "\n",
    "$q_{\\pi}(s,a) = \\mathbb E_{\\pi} [G_t | S_t = s, A_t = a]$ \n",
    "\n",
    "### ìµœì  ê°€ì¹˜ (Optimal Value)\n",
    "\n",
    "ìƒíƒœ s ì—ì„œ ê°€ì¥ í° Q-valueì´ë‹¤.       \n",
    "\n",
    "$q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a)$  \n",
    "\n",
    "### ìµœì  ì •ì±… (Optimal Policy)\n",
    "\n",
    "$q_*(s,a)$ê°€ ê²°ì • ë˜ë©´ $\\pi (s, a) = 1$ ì´ ëœë‹¤. ì¦‰ ìƒíƒœ sì¼ë•ŒëŠ” 100% ì•¡ì…˜ aë¥¼ ìˆ˜í–‰ í•œë‹¤.\n",
    "\n",
    "$ \\pi_*(s, a) = \\begin{cases}\n",
    "1 & \\text{if } a= \\text{argmax}_{a \\in A} q_\\star(s,a) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ì—ì´ì „íŠ¸ëŠ”      \n",
    "> ê° ìƒíƒœ(state)ì˜ ì•¡ì…˜(action)ë§ˆë‹¤  \n",
    "> Q-valueë¥¼ ê³„ì‚°í•œë‹¤.  \n",
    ">  \n",
    "> ì—ì´ì „íŠ¸ê°€\n",
    "> ìµœì ì˜ ì•¡ì…˜ì„ í•˜ê¸° ìœ„í•´ì„œëŠ”  \n",
    "> ê° ìƒíƒœì—ì„œ Q-valueì˜ ìµœê³ ê°’ì„ ê°€ì§„  \n",
    "> ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ë©´ ëœë‹¤!  \n",
    ">\n",
    "> ê·¸ëŸ°ë°  \n",
    "> Q-valueë¥¼ ì–´ë–»ê²Œ ì°¾ì§€?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë©”ëª¨ë¦¬ (Memory)\n",
    "\n",
    "ëœë¤ ì•¡ì…˜ì—ì„œ ì–»ì€ ë°ì´í„° ($s_t, a_t$) ë“¤ì„ ëª¬í…Œ ì¹´ë¥¼ë¡œì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ë©”ëª¨ë¦¬(memory)ì— ì €ì¥í•œë‹¤.\n",
    "\n",
    "$\\text{memory} = [(s_0, a_0), (s_1, a_1), \\cdots]$\n",
    "\n",
    "### Monte Carlo (ëª¬í…Œ ì¹´ë¥¼ë¡œ)\n",
    "\n",
    "'ì–¼ìŒí˜¸ìˆ˜' ì—ì„œ `S`(Start)ì—ì„œ ì‹œì‘í•´ì„œ `H`(Hole)ì— ë¹ ì§€ê±°ë‚˜  `G`(Goal)ì— ë„ì°©í•˜ë©´ ê²Œì„ì„ ëì´ë‚œë‹¤. ì´ ê²ƒì„ **ì—í”¼ì†Œë“œ(episode)** ë¼ê³  í•œë‹¤.  í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œê°€ ëë‚˜ë©´ ì—í”¼ì†Œë“œì—ì„œ ì§€ë‚˜ì™”ë˜ ìƒíƒœ(state)ì— ëŒ€í•´ì„œ Q-valueë¥¼ êµ¬í•œë‹¤. ê·¸ë¦¬ê³  ê³„ì†í•´ì„œ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰ ì‹œí‚¤ë©´ì„œ ì§€ë‚˜ì˜¨ ìƒíƒœ(state)ì— ëŒ€í•œ Q-valueë¥¼ ì—…ë°ì´íŠ¸ í•˜ë©´ ê²°êµ­ ìµœì ì˜ Q-valueë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ê²ƒì´ ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•(Monte Carlo method)ì´ë‹¤. ì´ë•Œ ì•¡ì…˜ì€ ëœë¤ìœ¼ë¡œ ê²°ì •í•œë‹¤.  \n",
    "\n",
    "$\\pi(0|\\cdot) = \\pi(1|\\cdot) = \\pi(2|\\cdot) = \\pi(3|\\cdot) = 0.25$\n",
    "\n",
    "í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œê°€ ëë‚˜ë©´ ì•„ë˜ì™€ ê°™ì´ Q-valueë¥¼ êµ¬í•œë‹¤.  \n",
    "\n",
    "$k$: Sample kë²ˆì§¸ ì—í”¼ì†Œë“œ  \n",
    "\n",
    "$\\begin{align}\n",
    "N(S_t, A_t) & \\leftarrow N(S_t, A_t) + 1 \\\\\n",
    "Q(S_t, A_t) & \\leftarrow Q(S_t, A_t) + \\dfrac{1}{N(S_t, A_t)} \\left( G_t - Q(S_t, A_t) \\right)  \n",
    "\\end{align}$\n",
    "\n",
    "#### Target  \n",
    "\n",
    "ëª©í‘œë¡œ í•˜ëŠ” ê°’ì´ë‹¤. ì—¬ê¸°ì„œ íƒ€ê²Ÿì€ $G_t$ì´ë‹¤.  \n",
    "\n",
    "#### Error (ë¸íƒ€)  \n",
    "\n",
    "ëª©í‘œê°’ê³¼ í˜„ì¬ê°’ê³¼ì˜ ì°¨ì´ë¥¼ $\\delta$ ë¼ê³  í•œë‹¤.   \n",
    "$\\delta_t = G_t - Q(S_t, A_t) $\n",
    "\n",
    "ê³„ì†í•´ì„œ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰ ì‹œí‚¤ë©´ì„œ $Q(S_t, A_t)$ì—ë‹¤ê°€ $\\delta_t$ ì˜ í‰ê· ì„ ì—…ë°ì´íŠ¸ í•˜ë©´ ê²°êµ­ $Q(s, a) \\rightarrow q_*(s,a)$ê°€ ëœë‹¤.  \n",
    "\n",
    "\n",
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ì•„í•˜! ìµœì ì˜ Q-valueë¥¼  ì°¾ì•„ ê°€ëŠ” ê²ƒì´ ê°•í™” í•™ìŠµ(Reinfocement Learning)í•˜ëŠ” ë°©ë²•ì´êµ¬ë‚˜!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ (Learning)\n",
    "\n",
    "ëª¬í…Œ ì¹´ë¥¼ë¡œë¥¼ ì´ìš©í•´ì„œ ìµœì ì˜ Q-valueë¥¼ ì°¾ì•„ë³´ì!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 7738.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_state = env.observation_space.n\n",
    "num_action = env.action_space.n\n",
    "num_episode = 5000\n",
    "\n",
    "# Q_table\n",
    "Q_table = np.zeros((num_state, num_action))\n",
    "# N_table\n",
    "N_table = np.zeros((num_state, num_action))\n",
    "# R_table\n",
    "R_table = np.zeros((num_state, num_action))\n",
    "\n",
    "for episode in tqdm(range(num_episode)):\n",
    "    memory = []    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(100):\n",
    "        action = env.action_space.sample()        \n",
    "        memory.append((state, action)) # trajectory\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        R_table[state][action] = reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done: # an episode finished  \n",
    "            #\n",
    "            # Monte Carlo policy evaluation\n",
    "            #\n",
    "            G_t = 0\n",
    "            for i in range(len(memory)):\n",
    "                gamma = 0.6 # discount factor\n",
    "                for j in range( i, len(memory) ):\n",
    "                    S_t = memory[j][0]\n",
    "                    A_t = memory[j][1]\n",
    "                    if i==j:\n",
    "                        G_t += R_table[S_t][A_t]\n",
    "                    else:\n",
    "                        G_t += gamma * R_table[S_t][A_t]\n",
    "                        gamma = gamma * gamma\n",
    "                S_t = memory[i][0]\n",
    "                A_t = memory[i][1]\n",
    "                N_table[S_t][A_t] += 1\n",
    "                Q_table[S_t][A_t] += (G_t - Q_table[S_t][A_t]) / N_table[S_t][A_t]\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•´ê²° (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Right)\n",
       "SFFF\n",
       "FHFH\n",
       "FFFH\n",
       "HFF\u001b[41mG\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ğŸ‰ğŸ‘ ì„±ê³µ! ğŸºğŸ¥‡\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q_table[state]) # Optimal Policy\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done and state == 15:\n",
    "        print('\\n ğŸ‰ğŸ‘ ì„±ê³µ! ğŸºğŸ¥‡')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

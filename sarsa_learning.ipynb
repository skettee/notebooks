{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA Learning\n",
    "\n",
    "Date: 2019-09-25  \n",
    "Author: skettee  \n",
    "Categories: Reinforcement Learning, Temporal-Difference Learning   \n",
    "Tags: Environment, Agent, State, Action, Reward, Policy, Q-value, SARSA, 𝜀-greedy  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습(Reinforcement Learning)에서 사용하는 SARSA에 대해서 알아 보고 Gym에서 제공하는 문제를 해결하기 위한 알고리듬을 만들어 보자.  \n",
    "<!--more-->\n",
    "\n",
    "실제로 돌려 보고 싶으면 구글 코랩으로 ~  \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skettee/notebooks/blob/master/sarsa_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 (Problem)\n",
    "\n",
    "👤 상사\n",
    "\n",
    "> 몬테 카를로와 같이 한 판이 끝날때 까지 기다리지 않고        \n",
    "> 한 칸 앞으로만 가도 학습이 되는게 있다고 하네?       \n",
    "> 아래 체육관(Gym)에 가서      \n",
    "> '얼음 호수8X8' 문제를 그걸로 풀어 보게          \n",
    ">\n",
    "> https://gym.openai.com/envs/FrozenLake8x8-v0/\n",
    "\n",
    "⚙️ 엔지니어\n",
    "\n",
    "> 또 다른 알고리듬이 등장 하겠군...  \n",
    "> 배움에는 끝이 없구냥...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 분석 (Problem Anaysis)\n",
    "\n",
    "일단 Gym을 돌려 보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output, Pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Right)\n",
       "SFFFFFFF\n",
       "FFFFFFFF\n",
       "FFF\u001b[41mH\u001b[0mFFFF\n",
       "FFFFFHFF\n",
       "FFFHFFFF\n",
       "FHHFFFHF\n",
       "FHFFHFHF\n",
       "FFFHFFFG\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 7 timesteps\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Environment\n",
    "#\n",
    "env = gym.make('FrozenLake8x8-v0', is_slippery=False) # 얼음위에서 미끄러지지 않도록 설정\n",
    "state = env.reset()\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "#\n",
    "# Agent\n",
    "#\n",
    "for step in range(100):\n",
    "    action =env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)    \n",
    "    state = next_state\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 (Environment)\n",
    "\n",
    "'얼음호수8X8' 의 환경은 64개의 상태(State)와 4개의 액션(Action)으로 구성 되어 있다.  \n",
    "\n",
    "### 액션 (Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'얼음호수8X8' 에서는 4개의 액션이 존재한다. 그리고 각각의 액션이 번호로 지정 되어 있다.\n",
    "\n",
    "$A = \\{0, 1, 2, 3\\}$   \n",
    "\n",
    "Num\t| Action\n",
    "----|----\n",
    "0 |\t왼쪽으로 이동 (Move Left)\n",
    "1 |\t아래로 이동 (Move Down)\n",
    "2 |\t오른쪽으로 이동 (Move Right)\n",
    "3 |\t위로 이동 (Move Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상태 (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'얼음호수8X8'의 상태(State) $S$는 다음과 같이 각 상태가 0과 63까지 번호로 지정되어 있다.\n",
    "\n",
    "$S = \\{0, 1, \\cdots , 63\\}$   \n",
    "\n",
    "$\\begin{vmatrix}\n",
    "0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\\n",
    "8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\\\\n",
    "16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 \\\\\n",
    "24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 \\\\\n",
    "32 & 33 & 34 & 35 & 36 & 37 & 38 & 39 \\\\\n",
    "40 & 41 & 42 & 43 & 44 & 45 & 46 & 47 \\\\\n",
    "48 & 49 & 50 & 51 & 52 & 53 & 54 & 55 \\\\\n",
    "56 & 57 & 58 & 59 & 60 & 61 & 62 & 63\n",
    "\\end{vmatrix}$\n",
    "\n",
    "그리고 각 상태마다 액션(action), 확률(probability), 다음 상태(next state), 보상(reward), 종료(done)가 `{action: [(probability, nextstate, reward, done)]}` 형식으로 정의 되어 있다.   \n",
    "\n",
    "에피소드가 종료되는 상태를 기록해 놓자  \n",
    "\n",
    "$S_{\\text{terminate}} = \\{19, 29, 35, 41, 42, 49, 52, 54, 59, 63\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보상 (Reward)\n",
    "\n",
    "55번의 상태를 까보자  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 54, 0.0, True)],\n",
       " 1: [(1.0, 63, 1.0, True)],\n",
       " 2: [(1.0, 55, 0.0, False)],\n",
       " 3: [(1.0, 47, 0.0, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63번 상태로 이동하면 $R = 1.0$ 을 얻고 에피소드가 끝이난다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 (Agent)\n",
    "\n",
    "에이전트는 환경에서 부터 주어진 상태($S$), 액션($A$), 보상($R$)을 가지고 가치(Q-value)를 계산하고 정책(Policy)을 수립해서 최적의 액션을 수행해야 한다.  \n",
    "\n",
    "### 정책 (Policy)\n",
    "\n",
    "에이전트가 주어진 상태에서 액션을 선택하는 확률이다.  \n",
    "\n",
    "$\\pi (a|s) = \\mathbb P[A_t = a | S_t = s]$ \n",
    "\n",
    "### 상태 가치 (State Value)\n",
    "\n",
    "상태 가치는 상태 s 에서 기대되는 미래 보상의 합이다. 여기서 $G_t$는 벨만 방정식(Bellman Equation)에 의해서 보상(Reward)값과 다음 스텝의 상태 가치로 분해 할 수 있다.  \n",
    "\n",
    "$\\begin{align}\n",
    "v_{\\pi}(s) & = \\mathbb E_{\\pi} [G_t | S_t = s] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) | S_t = s] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s] \n",
    "\\end{align}$ \n",
    "\n",
    "### Q-value (State Action Value)\n",
    "\n",
    "Q-value는 상태 s 에서 액션 a 를 수행할 경우에 기대되는 미래 보상의 합이다.  여기서 $G_t$는 벨만 방정식(Bellman Equation)에 의해서 보상(Reward)값과 다음 스텝의 Q-value로 분해 할 수 있다.   \n",
    "\n",
    "$\\begin{align}\n",
    "q_{\\pi}(s,a) & = \\mathbb E_{\\pi} [G_t | S_t = s, A_t = a] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s, A_t = a] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) | S_t = s, A_t = a] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb E_{\\pi} [R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \n",
    "\\end{align}$ \n",
    "\n",
    "### 최적 가치 (Optimal Value)\n",
    "\n",
    "상태 s 에서 가장 큰 Q-value이다.       \n",
    "\n",
    "$q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a)$  \n",
    "\n",
    "### 최적 정책 (Optimal Policy)\n",
    "\n",
    "$q_*(s,a)$가 결정 되면 $\\pi (a|s) = 1$ 이 된다. 즉 상태 s일때는 100% 액션 a를 수행 한다.\n",
    "\n",
    "$\\pi_*(a|s) = \\begin{cases}\n",
    "1 & \\text{if } a= \\text{argmax}_{a \\in A} q_\\star(s,a) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "⚙️ 엔지니어\n",
    "\n",
    "> 에이전트는      \n",
    "> 각 상태(state)의 액션(action)마다  \n",
    "> Q-value를 계산한다.  \n",
    ">  \n",
    "> 몬테 카를로 방법은   \n",
    "> 에피소드가 끝나고 나서 $G_t$를 이용해서 Q-value를 계산했다.  \n",
    "> \n",
    "> 그런데  \n",
    "> $G_t$ 대신에 $R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1})$을 이용하면  \n",
    "> 한 스텝만 이동해도 Q-value를 계산할 수 있는  \n",
    "> 신묘한 알고리듬이 된다.  \n",
    "> 그것은 바로...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 살사 (SARSA)\n",
    "\n",
    "State-Action-Reward-State-Action의 줄임말로서 미래 보상의 합 $G_t$ 대신에 보상(Reward)값과 다음 스텝의 Q-value를 이용해서 Q-value를 최적화 하는 알고리듬이다.  \n",
    "\n",
    "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right)$\n",
    "\n",
    "$\\alpha$ : learning rate  \n",
    "$\\gamma$ : 디스카운트 (discount factor)  \n",
    "\n",
    "#### Target  \n",
    "목표로 하는 값이다. 여기서 타겟은 $R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})$이다.  \n",
    "\n",
    "#### Error (델타)  \n",
    "목표값과 현재값과의 차이를 $\\delta$ 라고 한다.   \n",
    "\n",
    "$\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$\n",
    "\n",
    "계속해서 에피소드를 실행 시키면서 $Q(S_t, A_t)$에다가 $\\alpha * \\delta_t$ 를 업데이트 하면 결국 $Q(s, a) \\rightarrow q_*(s,a)$가 된다.  \n",
    "\n",
    "### 𝜀-greedy  \n",
    "\n",
    "다음 스텝의 액션을 선택하는 경우에 욕심쟁이(greedy)처럼 최고의 Q-value의 액션만을 선택하면 오류가 발생할 수 있다. 따라서 적당히 램덤하게 액션을 선택하는 경우를 추가한 것이 𝜀-greedy이다.  \n",
    "\n",
    "$\\pi \\leftarrow \\epsilon \\text{-greedy(Q)}$\n",
    "\n",
    "𝜀의 확률로 랜덤 액션을 수행하고 (1-𝜀)의 확률로 Q-value가 가장 큰 액션을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 (Learning)\n",
    "\n",
    "살사를 이용해서 최적의 Q-value를 찾아보자!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3752.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_state = env.observation_space.n\n",
    "num_action = env.action_space.n\n",
    "num_episode = 1000\n",
    "\n",
    "# Initialize Q_table \n",
    "Q_table = np.random.uniform(low=0.0, high=0.00000001, size=(num_state, num_action))\n",
    "# Zero for terminate states\n",
    "for s in [19, 29, 35, 41, 42, 49, 52, 54, 59, 63]:\n",
    "    Q_table[s] = 0\n",
    "\n",
    "for episode in tqdm(range(num_episode)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    # Hyper parameter\n",
    "    epsilon = 0.3\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    \n",
    "    if np.random.uniform() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q_table[state])\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if np.random.uniform() < epsilon:\n",
    "            next_action = env.action_space.sample()\n",
    "        else:\n",
    "            next_action = np.argmax(Q_table[next_state])\n",
    "        \n",
    "        target = reward + gamma*Q_table[next_state, next_action] \n",
    "        delta = target - Q_table[state][action]\n",
    "        Q_table[state][action] += alpha * delta\n",
    "        state, action = next_state, next_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해결 (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Down)\n",
       "SFFFFFFF\n",
       "FFFFFFFF\n",
       "FFFHFFFF\n",
       "FFFFFHFF\n",
       "FFFHFFFF\n",
       "FHHFFFHF\n",
       "FHFFHFHF\n",
       "FFFHFFF\u001b[41mG\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 🎉👍 성공! 🍺🥇\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q_table[state]) # Optimal Policy\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done and state == 63:\n",
    "        print('\\n 🎉👍 성공! 🍺🥇')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA Learning\n",
    "\n",
    "Date: 2019-09-25  \n",
    "Author: skettee  \n",
    "Categories: Reinforcement Learning, Temporal-Difference Learning   \n",
    "Tags: Environment, Agent, State, Action, Reward, Policy, Q-value, SARSA, ğœ€-greedy  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°•í™” í•™ìŠµ(Reinforcement Learning)ì—ì„œ ì‚¬ìš©í•˜ëŠ” SARSAì— ëŒ€í•´ì„œ ì•Œì•„ ë³´ê³  Gymì—ì„œ ì œê³µí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ë“¬ì„ ë§Œë“¤ì–´ ë³´ì.  \n",
    "<!--more-->\n",
    "\n",
    "ì‹¤ì œë¡œ ëŒë ¤ ë³´ê³  ì‹¶ìœ¼ë©´ êµ¬ê¸€ ì½”ë©ìœ¼ë¡œ ~  \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skettee/notebooks/blob/master/sarsa_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¬¸ì œ (Problem)\n",
    "\n",
    "ğŸ‘¤ ìƒì‚¬\n",
    "\n",
    "> ëª¬í…Œ ì¹´ë¥¼ë¡œì™€ ê°™ì´ í•œ íŒì´ ëë‚ ë•Œ ê¹Œì§€ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³         \n",
    "> í•œ ì¹¸ ì•ìœ¼ë¡œë§Œ ê°€ë„ í•™ìŠµì´ ë˜ëŠ”ê²Œ ìˆë‹¤ê³  í•˜ë„¤?       \n",
    "> ì•„ë˜ ì²´ìœ¡ê´€(Gym)ì— ê°€ì„œ      \n",
    "> 'ì–¼ìŒ í˜¸ìˆ˜8X8' ë¬¸ì œë¥¼ ê·¸ê±¸ë¡œ í’€ì–´ ë³´ê²Œ          \n",
    ">\n",
    "> https://gym.openai.com/envs/FrozenLake8x8-v0/\n",
    "\n",
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ë˜ ë‹¤ë¥¸ ì•Œê³ ë¦¬ë“¬ì´ ë“±ì¥ í•˜ê² êµ°...  \n",
    "> ë°°ì›€ì—ëŠ” ëì´ ì—†êµ¬ëƒ¥...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¬¸ì œ ë¶„ì„ (Problem Anaysis)\n",
    "\n",
    "ì¼ë‹¨ Gymì„ ëŒë ¤ ë³´ì!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output, Pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Right)\n",
       "SFFFFFFF\n",
       "FFFFFFFF\n",
       "FFF\u001b[41mH\u001b[0mFFFF\n",
       "FFFFFHFF\n",
       "FFFHFFFF\n",
       "FHHFFFHF\n",
       "FHFFHFHF\n",
       "FFFHFFFG\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 7 timesteps\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Environment\n",
    "#\n",
    "env = gym.make('FrozenLake8x8-v0', is_slippery=False) # ì–¼ìŒìœ„ì—ì„œ ë¯¸ë„ëŸ¬ì§€ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "state = env.reset()\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "#\n",
    "# Agent\n",
    "#\n",
    "for step in range(100):\n",
    "    action =env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)    \n",
    "    state = next_state\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ (Environment)\n",
    "\n",
    "'ì–¼ìŒí˜¸ìˆ˜8X8' ì˜ í™˜ê²½ì€ 64ê°œì˜ ìƒíƒœ(State)ì™€ 4ê°œì˜ ì•¡ì…˜(Action)ìœ¼ë¡œ êµ¬ì„± ë˜ì–´ ìˆë‹¤.  \n",
    "\n",
    "### ì•¡ì…˜ (Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ì–¼ìŒí˜¸ìˆ˜8X8' ì—ì„œëŠ” 4ê°œì˜ ì•¡ì…˜ì´ ì¡´ì¬í•œë‹¤. ê·¸ë¦¬ê³  ê°ê°ì˜ ì•¡ì…˜ì´ ë²ˆí˜¸ë¡œ ì§€ì • ë˜ì–´ ìˆë‹¤.\n",
    "\n",
    "$A = \\{0, 1, 2, 3\\}$   \n",
    "\n",
    "Num\t| Action\n",
    "----|----\n",
    "0 |\tì™¼ìª½ìœ¼ë¡œ ì´ë™ (Move Left)\n",
    "1 |\tì•„ë˜ë¡œ ì´ë™ (Move Down)\n",
    "2 |\tì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ (Move Right)\n",
    "3 |\tìœ„ë¡œ ì´ë™ (Move Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìƒíƒœ (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ì–¼ìŒí˜¸ìˆ˜8X8'ì˜ ìƒíƒœ(State) $S$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê° ìƒíƒœê°€ 0ê³¼ 63ê¹Œì§€ ë²ˆí˜¸ë¡œ ì§€ì •ë˜ì–´ ìˆë‹¤.\n",
    "\n",
    "$S = \\{0, 1, \\cdots , 63\\}$   \n",
    "\n",
    "$\\begin{vmatrix}\n",
    "0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\\n",
    "8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\\\\n",
    "16 & 17 & 18 & 19 & 20 & 21 & 22 & 23 \\\\\n",
    "24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 \\\\\n",
    "32 & 33 & 34 & 35 & 36 & 37 & 38 & 39 \\\\\n",
    "40 & 41 & 42 & 43 & 44 & 45 & 46 & 47 \\\\\n",
    "48 & 49 & 50 & 51 & 52 & 53 & 54 & 55 \\\\\n",
    "56 & 57 & 58 & 59 & 60 & 61 & 62 & 63\n",
    "\\end{vmatrix}$\n",
    "\n",
    "ê·¸ë¦¬ê³  ê° ìƒíƒœë§ˆë‹¤ ì•¡ì…˜(action), í™•ë¥ (probability), ë‹¤ìŒ ìƒíƒœ(next state), ë³´ìƒ(reward), ì¢…ë£Œ(done)ê°€ `{action: [(probability, nextstate, reward, done)]}` í˜•ì‹ìœ¼ë¡œ ì •ì˜ ë˜ì–´ ìˆë‹¤.   \n",
    "\n",
    "ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ëŠ” ìƒíƒœë¥¼ ê¸°ë¡í•´ ë†“ì  \n",
    "\n",
    "$S_{\\text{terminate}} = \\{19, 29, 35, 41, 42, 49, 52, 54, 59, 63\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë³´ìƒ (Reward)\n",
    "\n",
    "55ë²ˆì˜ ìƒíƒœë¥¼ ê¹Œë³´ì  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 54, 0.0, True)],\n",
       " 1: [(1.0, 63, 1.0, True)],\n",
       " 2: [(1.0, 55, 0.0, False)],\n",
       " 3: [(1.0, 47, 0.0, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63ë²ˆ ìƒíƒœë¡œ ì´ë™í•˜ë©´ $R = 1.0$ ì„ ì–»ê³  ì—í”¼ì†Œë“œê°€ ëì´ë‚œë‹¤.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—ì´ì „íŠ¸ (Agent)\n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” í™˜ê²½ì—ì„œ ë¶€í„° ì£¼ì–´ì§„ ìƒíƒœ($S$), ì•¡ì…˜($A$), ë³´ìƒ($R$)ì„ ê°€ì§€ê³  ê°€ì¹˜(Q-value)ë¥¼ ê³„ì‚°í•˜ê³  ì •ì±…(Policy)ì„ ìˆ˜ë¦½í•´ì„œ ìµœì ì˜ ì•¡ì…˜ì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.  \n",
    "\n",
    "### ì •ì±… (Policy)\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì£¼ì–´ì§„ ìƒíƒœì—ì„œ ì•¡ì…˜ì„ ì„ íƒí•˜ëŠ” í™•ë¥ ì´ë‹¤.  \n",
    "\n",
    "$\\pi (a|s) = \\mathbb P[A_t = a | S_t = s]$ \n",
    "\n",
    "### ìƒíƒœ ê°€ì¹˜ (State Value)\n",
    "\n",
    "ìƒíƒœ ê°€ì¹˜ëŠ” ìƒíƒœ s ì—ì„œ ê¸°ëŒ€ë˜ëŠ” ë¯¸ë˜ ë³´ìƒì˜ í•©ì´ë‹¤. ì—¬ê¸°ì„œ $G_t$ëŠ” ë²¨ë§Œ ë°©ì •ì‹(Bellman Equation)ì— ì˜í•´ì„œ ë³´ìƒ(Reward)ê°’ê³¼ ë‹¤ìŒ ìŠ¤í…ì˜ ìƒíƒœ ê°€ì¹˜ë¡œ ë¶„í•´ í•  ìˆ˜ ìˆë‹¤.  \n",
    "\n",
    "$\\begin{align}\n",
    "v_{\\pi}(s) & = \\mathbb E_{\\pi} [G_t | S_t = s] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) | S_t = s] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma G_{t+1} | S_t = s] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s] \n",
    "\\end{align}$ \n",
    "\n",
    "### Q-value (State Action Value)\n",
    "\n",
    "Q-valueëŠ” ìƒíƒœ s ì—ì„œ ì•¡ì…˜ a ë¥¼ ìˆ˜í–‰í•  ê²½ìš°ì— ê¸°ëŒ€ë˜ëŠ” ë¯¸ë˜ ë³´ìƒì˜ í•©ì´ë‹¤.  ì—¬ê¸°ì„œ $G_t$ëŠ” ë²¨ë§Œ ë°©ì •ì‹(Bellman Equation)ì— ì˜í•´ì„œ ë³´ìƒ(Reward)ê°’ê³¼ ë‹¤ìŒ ìŠ¤í…ì˜ Q-valueë¡œ ë¶„í•´ í•  ìˆ˜ ìˆë‹¤.   \n",
    "\n",
    "$\\begin{align}\n",
    "q_{\\pi}(s,a) & = \\mathbb E_{\\pi} [G_t | S_t = s, A_t = a] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots | S_t = s, A_t = a] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) | S_t = s, A_t = a] \\\\\n",
    "& = \\mathbb E_{\\pi} [R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a] \\\\\n",
    "&= \\mathbb E_{\\pi} [R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \n",
    "\\end{align}$ \n",
    "\n",
    "### ìµœì  ê°€ì¹˜ (Optimal Value)\n",
    "\n",
    "ìƒíƒœ s ì—ì„œ ê°€ì¥ í° Q-valueì´ë‹¤.       \n",
    "\n",
    "$q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a)$  \n",
    "\n",
    "### ìµœì  ì •ì±… (Optimal Policy)\n",
    "\n",
    "$q_*(s,a)$ê°€ ê²°ì • ë˜ë©´ $\\pi (a|s) = 1$ ì´ ëœë‹¤. ì¦‰ ìƒíƒœ sì¼ë•ŒëŠ” 100% ì•¡ì…˜ aë¥¼ ìˆ˜í–‰ í•œë‹¤.\n",
    "\n",
    "$\\pi_*(a|s) = \\begin{cases}\n",
    "1 & \\text{if } a= \\text{argmax}_{a \\in A} q_\\star(s,a) \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "âš™ï¸ ì—”ì§€ë‹ˆì–´\n",
    "\n",
    "> ì—ì´ì „íŠ¸ëŠ”      \n",
    "> ê° ìƒíƒœ(state)ì˜ ì•¡ì…˜(action)ë§ˆë‹¤  \n",
    "> Q-valueë¥¼ ê³„ì‚°í•œë‹¤.  \n",
    ">  \n",
    "> ëª¬í…Œ ì¹´ë¥¼ë¡œ ë°©ë²•ì€   \n",
    "> ì—í”¼ì†Œë“œê°€ ëë‚˜ê³  ë‚˜ì„œ $G_t$ë¥¼ ì´ìš©í•´ì„œ Q-valueë¥¼ ê³„ì‚°í–ˆë‹¤.  \n",
    "> \n",
    "> ê·¸ëŸ°ë°  \n",
    "> $G_t$ ëŒ€ì‹ ì— $R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1})$ì„ ì´ìš©í•˜ë©´  \n",
    "> í•œ ìŠ¤í…ë§Œ ì´ë™í•´ë„ Q-valueë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ”  \n",
    "> ì‹ ë¬˜í•œ ì•Œê³ ë¦¬ë“¬ì´ ëœë‹¤.  \n",
    "> ê·¸ê²ƒì€ ë°”ë¡œ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚´ì‚¬ (SARSA)\n",
    "\n",
    "State-Action-Reward-State-Actionì˜ ì¤„ì„ë§ë¡œì„œ ë¯¸ë˜ ë³´ìƒì˜ í•© $G_t$ ëŒ€ì‹ ì— ë³´ìƒ(Reward)ê°’ê³¼ ë‹¤ìŒ ìŠ¤í…ì˜ Q-valueë¥¼ ì´ìš©í•´ì„œ Q-valueë¥¼ ìµœì í™” í•˜ëŠ” ì•Œê³ ë¦¬ë“¬ì´ë‹¤.  \n",
    "\n",
    "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right)$\n",
    "\n",
    "$\\alpha$ : learning rate  \n",
    "$\\gamma$ : ë””ìŠ¤ì¹´ìš´íŠ¸ (discount factor)  \n",
    "\n",
    "#### Target  \n",
    "ëª©í‘œë¡œ í•˜ëŠ” ê°’ì´ë‹¤. ì—¬ê¸°ì„œ íƒ€ê²Ÿì€ $R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})$ì´ë‹¤.  \n",
    "\n",
    "#### Error (ë¸íƒ€)  \n",
    "ëª©í‘œê°’ê³¼ í˜„ì¬ê°’ê³¼ì˜ ì°¨ì´ë¥¼ $\\delta$ ë¼ê³  í•œë‹¤.   \n",
    "\n",
    "$\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$\n",
    "\n",
    "ê³„ì†í•´ì„œ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰ ì‹œí‚¤ë©´ì„œ $Q(S_t, A_t)$ì—ë‹¤ê°€ $\\alpha * \\delta_t$ ë¥¼ ì—…ë°ì´íŠ¸ í•˜ë©´ ê²°êµ­ $Q(s, a) \\rightarrow q_*(s,a)$ê°€ ëœë‹¤.  \n",
    "\n",
    "### ğœ€-greedy  \n",
    "\n",
    "ë‹¤ìŒ ìŠ¤í…ì˜ ì•¡ì…˜ì„ ì„ íƒí•˜ëŠ” ê²½ìš°ì— ìš•ì‹¬ìŸì´(greedy)ì²˜ëŸ¼ ìµœê³ ì˜ Q-valueì˜ ì•¡ì…˜ë§Œì„ ì„ íƒí•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì ë‹¹íˆ ë¨ë¤í•˜ê²Œ ì•¡ì…˜ì„ ì„ íƒí•˜ëŠ” ê²½ìš°ë¥¼ ì¶”ê°€í•œ ê²ƒì´ ğœ€-greedyì´ë‹¤.  \n",
    "\n",
    "$\\pi \\leftarrow \\epsilon \\text{-greedy(Q)}$\n",
    "\n",
    "ğœ€ì˜ í™•ë¥ ë¡œ ëœë¤ ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ê³  (1-ğœ€)ì˜ í™•ë¥ ë¡œ Q-valueê°€ ê°€ì¥ í° ì•¡ì…˜ì„ ìˆ˜í–‰í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•™ìŠµ (Learning)\n",
    "\n",
    "ì‚´ì‚¬ë¥¼ ì´ìš©í•´ì„œ ìµœì ì˜ Q-valueë¥¼ ì°¾ì•„ë³´ì!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 3752.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_state = env.observation_space.n\n",
    "num_action = env.action_space.n\n",
    "num_episode = 1000\n",
    "\n",
    "# Initialize Q_table \n",
    "Q_table = np.random.uniform(low=0.0, high=0.00000001, size=(num_state, num_action))\n",
    "# Zero for terminate states\n",
    "for s in [19, 29, 35, 41, 42, 49, 52, 54, 59, 63]:\n",
    "    Q_table[s] = 0\n",
    "\n",
    "for episode in tqdm(range(num_episode)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    # Hyper parameter\n",
    "    epsilon = 0.3\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    \n",
    "    if np.random.uniform() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q_table[state])\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if np.random.uniform() < epsilon:\n",
    "            next_action = env.action_space.sample()\n",
    "        else:\n",
    "            next_action = np.argmax(Q_table[next_state])\n",
    "        \n",
    "        target = reward + gamma*Q_table[next_state, next_action] \n",
    "        delta = target - Q_table[state][action]\n",
    "        Q_table[state][action] += alpha * delta\n",
    "        state, action = next_state, next_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í•´ê²° (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  (Down)\n",
       "SFFFFFFF\n",
       "FFFFFFFF\n",
       "FFFHFFFF\n",
       "FFFFFHFF\n",
       "FFFHFFFF\n",
       "FHHFFFHF\n",
       "FHFFHFHF\n",
       "FFFHFFF\u001b[41mG\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ğŸ‰ğŸ‘ ì„±ê³µ! ğŸºğŸ¥‡\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "# Initial world display\n",
    "world = env.render(mode='ansi')\n",
    "display(Pretty(world))\n",
    "sleep(0.5)\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q_table[state]) # Optimal Policy\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # updated world display\n",
    "    world = env.render(mode='ansi')\n",
    "    clear_output(wait=True)\n",
    "    display(Pretty(world))\n",
    "    sleep(0.5)\n",
    "    \n",
    "    if done and state == 63:\n",
    "        print('\\n ğŸ‰ğŸ‘ ì„±ê³µ! ğŸºğŸ¥‡')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
